{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is pretty much just staging for preparing the synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aimeri/miniforge3/envs/smollm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import random\n",
    "import re\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import orjson\n",
    "from tqdm import tqdm\n",
    "import lmstudio as lms\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't have enough quality data for characters using Character card definitions, we will use the information contained there to create a synthetic dataset using an LLM to roleplay as the character to give expositional information via dialog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_JSON_PATH = \"/Users/aimeri/Downloads/Cricket.json\"\n",
    "MAX_LENGTH = 2048\n",
    "MODEL_NAME = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "DEVICE = \"mps\" # for Apple Silicon, for GPU nvidia usage use \"cuda\" or \"cpu\" for CPU usage\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Template library for generating synthetic data\n",
    "# Each entry is (mode, prompt-template) where fields in single curly braces {field} \n",
    "# will be filled with card attributes (name, description, etc.) or template variables\n",
    "# (question, topic, user_prompt, etc.) using str.format().\n",
    "# --------------------------------------------------------------------------------------\n",
    "_TEMPLATES: List[tuple[str, str]] = [\n",
    "    (\n",
    "        \"short_qa\",\n",
    "        textwrap.dedent(\n",
    "            \"\"\"\n",
    "            You are {name}. Answer the question in first person and stay in character.\n",
    "            Q: {question}\n",
    "            A:\"\"\",\n",
    "        ).strip(),\n",
    "    ),\n",
    "    (\n",
    "        \"narration\",\n",
    "        \"Write one paragraph describing {name} entering a room from {name}'s perspective. Mention at least one physical trait in a subtle way.\",\n",
    "    ),\n",
    "    (\n",
    "        \"monologue\",\n",
    "        \"In two sentences let {name} reflect on {topic} while subtly referencing {fact}.\",\n",
    "    ),\n",
    "    (\n",
    "        \"dialogue_turn\",\n",
    "        \"User: {user_prompt}\\n### {name}:\",\n",
    "    ),\n",
    "    (\n",
    "        \"character_response\",\n",
    "        \"{user_prompt}\",\n",
    "    ),\n",
    "    (\n",
    "        \"internal_thought\",\n",
    "        \"Write {name}'s internal thoughts about {situation} in first person.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Fallback lists for variables referenced above\n",
    "_DEFAULT_QUESTIONS = [\n",
    "    \"What drives you?\",\n",
    "    \"Describe your greatest fear.\",\n",
    "    \"Why do you keep adventuring despite the risks?\",\n",
    "    \"Do you believe people can change their fate?\",\n",
    "]\n",
    "_DEFAULT_TOPICS = [\n",
    "    \"the nature of courage\",\n",
    "    \"loneliness on the road\",\n",
    "    \"the weight of leadership\",\n",
    "    \"how the stars guide travellers\",\n",
    "]\n",
    "_DEFAULT_USER_PROMPTS = [\n",
    "    \"Tell me about your homeland.\",\n",
    "    \"How did you acquire your skills?\",\n",
    "    \"What's your next goal?\",\n",
    "    \"Do you trust the new companion?\",\n",
    "    \"What's your biggest regret?\",\n",
    "    \"How do you handle failure?\",\n",
    "    \"Do you ever feel lust?\",\n",
    "    \"What's your favorite food?\",\n",
    "    \"What's your favorite drink?\",\n",
    "    \"What's your favorite color?\",\n",
    "    \"What's your favorite animal?\",\n",
    "    \"What's your favorite book?\",\n",
    "    \"What's your favorite activity?\",\n",
    "    \"Have you ever been in love?\",\n",
    "    \"Have you lost someone dear to you?\",\n",
    "]\n",
    "\n",
    "_DEFAULT_SITUATIONS = [\n",
    "    \"facing an impossible challenge\",\n",
    "    \"meeting an old enemy\",\n",
    "    \"discovering a hidden truth\",\n",
    "    \"making a difficult choice\",\n",
    "    \"losing something important\",\n",
    "    \"facing certain death\",\n",
    "    \"experienceing pleasure\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------\n",
    "# LM Studio helper â€“ runs the CLI and returns the raw string output.\n",
    "# --------------------------------------------------------------------------------------\n",
    "async def _lmstudio_chat(prompt: str, max_tokens: int = 160, temperature: float = 0.8, top_p: float = 0.9) -> str:\n",
    "    \"\"\"Call LM Studio CLI asynchronously and return the generated text.\"\"\"\n",
    "\n",
    "    model = lms.llm()\n",
    "\n",
    "    # # Might need to add a system prompt to the model\n",
    "\n",
    "    # # Create a chat with an initial system prompt.\n",
    "    # chat = lms.Chat(\"You are a resident AI philosopher.\")\n",
    "\n",
    "    # # Build the chat context by adding messages of relevant types.\n",
    "    # chat.add_user_message(\"What is the meaning of life?\")\n",
    "    # # result = model.respond(chat)\n",
    "\n",
    "    return model.respond(prompt, config={\n",
    "        \"temperature\": temperature,\n",
    "        \"topPSampling\": top_p,\n",
    "        \"maxTokens\": max_tokens,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------\n",
    "# Card helpers\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "_KEEP_FIELDS = {\n",
    "    \"name\",\n",
    "    \"description\",\n",
    "    \"personality\",\n",
    "    \"mes_example\",\n",
    "    \"scenario\",\n",
    "}\n",
    "\n",
    "\n",
    "def _load_card(path: Path) -> Dict[str, str]:\n",
    "    \"\"\"Load a SillyTavern card (.json) and keep only whitelisted keys.\"\"\"\n",
    "    raw = orjson.loads(path.read_bytes())\n",
    "    card = {k: v for k, v in raw.items() if k in _KEEP_FIELDS and isinstance(v, str)}\n",
    "    # Normalise whitespace\n",
    "    for k, v in card.items():\n",
    "        card[k] = re.sub(r\"\\s+\", \" \", v).strip()\n",
    "    return card\n",
    "\n",
    "\n",
    "def _make_card_block(card: Dict[str, str]) -> str:\n",
    "    \"\"\"Return the canonical <CHAR_CARD> block used as system prompt.\"\"\"\n",
    "    lines = [\"### <CHAR_CARD>\"]\n",
    "    lines.append(f\"Name: {card.get('name', 'Unknown')}\")\n",
    "    if \"species\" in card:\n",
    "        lines.append(f\"Species: {card['species']}\")\n",
    "    if \"age\" in card:\n",
    "        lines.append(f\"Age: {card['age']}\")\n",
    "    if \"gender\" in card:\n",
    "        lines.append(f\"Gender: {card['gender']}\")\n",
    "\n",
    "    for field in (\"description\", \"scenario\", \"personality\", \"first_person\"):\n",
    "        if field in card:\n",
    "            pretty = card[field].replace(\"\\n\", \" \")\n",
    "            lines.append(f\"{field.capitalize()}: {pretty}\")\n",
    "\n",
    "    lines.append(\"<|endofcard|>\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------\n",
    "# Synthetic sample generation\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "from lmstudio import PredictionResult\n",
    "\n",
    "\n",
    "def _fill_template(template: str, card: Dict[str, str]) -> str:\n",
    "    \"\"\"Replace placeholders inside a template string.\"\"\"\n",
    "\n",
    "    def _rand(lst: List[str]):\n",
    "        return random.choice(lst)\n",
    "\n",
    "    # Create a combined dictionary with card attributes and template variables\n",
    "    format_dict = dict(card)  # Start with card attributes\n",
    "    \n",
    "    # Add template variables\n",
    "    template_vars = {\n",
    "        \"question\": _rand(_DEFAULT_QUESTIONS),\n",
    "        \"topic\": _rand(_DEFAULT_TOPICS),\n",
    "        \"fact\": card.get(\"description\", \"your past\"),\n",
    "        \"user_prompt\": _rand(_DEFAULT_USER_PROMPTS),\n",
    "        \"situation\": _rand(_DEFAULT_SITUATIONS),\n",
    "    }\n",
    "    \n",
    "    # Merge template variables into format_dict\n",
    "    format_dict.update(template_vars)\n",
    "\n",
    "    try:\n",
    "        # Single-step formatting using str.format()\n",
    "        templated = template.format(**format_dict)\n",
    "        return templated.strip()\n",
    "    except KeyError as e:\n",
    "        print(f\"[warn] Missing key in template formatting: {e}\")\n",
    "        # Fallback with minimal required fields\n",
    "        try:\n",
    "            minimal_dict = {\"name\": card.get(\"name\", \"Unknown\")}\n",
    "            minimal_dict.update(template_vars)\n",
    "            return template.format(**minimal_dict).strip()\n",
    "        except:\n",
    "            return template.strip()\n",
    "\n",
    "\n",
    "async def _generate_for_card(\n",
    "    card_path: Path,\n",
    "    samples_per_card: int,\n",
    "    max_tokens: int,\n",
    "    temperature: float,\n",
    "    top_p: float,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Generate *samples_per_card* examples for a single character card.\"\"\"\n",
    "\n",
    "    card = _load_card(card_path)\n",
    "    card_block = _make_card_block(card)\n",
    "\n",
    "    out_samples: List[Dict[str, Any]] = []\n",
    "\n",
    "    # Synchronously generate samples\n",
    "    for _ in tqdm(range(samples_per_card)):\n",
    "        mode, template = random.choice(_TEMPLATES)\n",
    "        prompt = _fill_template(template, card)\n",
    "        \n",
    "        # Validate that all placeholders were filled\n",
    "        unfilled_placeholders = re.findall(r'\\{[^}]+\\}', prompt)\n",
    "        if unfilled_placeholders:\n",
    "            print(f\"[warn] Skipping sample with unfilled placeholders: {unfilled_placeholders}\")\n",
    "            continue\n",
    "\n",
    "        # Build final prompt: system card + user content (if any) â€“ we expect the\n",
    "        # model to answer as the assistant.\n",
    "        full_prompt = f\"{card_block}\\n\\n{prompt}\"\n",
    "\n",
    "        try:\n",
    "            reply: PredictionResult = await _lmstudio_chat(\n",
    "                prompt=full_prompt,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "            )\n",
    "            reply_content = reply.content\n",
    "\n",
    "            # Quick hard filters\n",
    "            if len(reply_content.split()) < 3:\n",
    "                continue\n",
    "            if len(reply_content.split()) > 420:\n",
    "                continue\n",
    "\n",
    "            # Build ChatML sample (system + user + assistant)\n",
    "            sample = {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": card_block},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                    {\"role\": \"assistant\", \"content\": reply_content},\n",
    "                ]\n",
    "            }\n",
    "            out_samples.append(sample)\n",
    "\n",
    "        except Exception as exc:\n",
    "            print(f\"[warn] generation failed: {exc}\")\n",
    "            continue\n",
    "\n",
    "    return out_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------\n",
    "# File helpers\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def _write_sharded_jsonl(samples: List[Dict[str, Any]], out_dir: Path, shard_size: int = 2000):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    shard_idx = 0\n",
    "    for i in range(0, len(samples), shard_size):\n",
    "        shard = samples[i : i + shard_size]\n",
    "        shard_path = out_dir / f\"synthetic_{shard_idx:03d}.jsonl\"\n",
    "        with shard_path.open(\"wb\") as f:\n",
    "            for ex in shard:\n",
    "                f.write(orjson.dumps(ex))\n",
    "                f.write(b\"\\n\")\n",
    "        shard_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to convert the Character card into a training set. Even the best writen cards would fall short of content and variety with enough repetition to avoid overfitting. So we take the information in them and use it to generate synthetic data based on the character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generating for Cricket.json ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [1:06:45<00:00, 10.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 400 samples. Writing shards â€¦\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_samples: List[Dict[str, Any]] = []\n",
    "\n",
    "# async def _driver():\n",
    "card_path = Path(CHAR_JSON_PATH)\n",
    "print(f\"\\n=== Generating for {card_path.name} ===\")\n",
    "samples = await _generate_for_card(\n",
    "    card_path,\n",
    "    samples_per_card=400,\n",
    "    max_tokens=400,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    ")\n",
    "all_samples.extend(samples)\n",
    "\n",
    "# asyncio.run(_driver())\n",
    "\n",
    "print(f\"\\nGenerated {len(all_samples)} samples. Writing shards â€¦\")\n",
    "_write_sharded_jsonl(all_samples, Path(\"data/synthetic_dataset\"))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our synthetic dataset we just need to massage it into a format that we can consume using the datasets library during finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for dataset preparation\n",
    "\n",
    "# Mask user and system tokens\n",
    "def mask_user_system_tokens(tokenizer, text: str):\n",
    "    \"\"\"Return (input_ids, labels) where labels for non-assistant tokens are â€‘100.\"\"\"\n",
    "    # Tokenise full dialogue first\n",
    "    input_ids = tokenizer(text, add_special_tokens=False)[\"input_ids\"]\n",
    "    labels = input_ids.copy()\n",
    "\n",
    "    # Everything up to and including the *last* assistant tag is user/system context.\n",
    "    assistant_tag = \"<|im_start|>assistant\"\n",
    "    tag_index = text.rfind(assistant_tag)\n",
    "\n",
    "    if tag_index != -1:\n",
    "        # Compute number of tokens before assistant starts\n",
    "        prefix_ids = tokenizer(text[: tag_index + len(assistant_tag)], add_special_tokens=False)[\n",
    "            \"input_ids\"\n",
    "        ]\n",
    "        labels[: len(prefix_ids)] = [-100] * len(prefix_ids)\n",
    "    else:\n",
    "        # Fallback: mask nothing if we cannot find the tag\n",
    "        labels = [-100] * len(labels)\n",
    "\n",
    "    return input_ids, labels\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Process example\n",
    "def process(example):\n",
    "    if \"messages\" not in example or not example[\"messages\"]:\n",
    "        # Return None to filter out malformed examples\n",
    "        return None\n",
    "\n",
    "    messages = example[\"messages\"]\n",
    "\n",
    "    # Ensure each element has the keys we expect\n",
    "    cleaned_messages = [\n",
    "        {\"role\": m[\"role\"], \"content\": m[\"content\"]}\n",
    "        for m in messages\n",
    "        if \"role\" in m and \"content\" in m\n",
    "    ]\n",
    "\n",
    "    if not cleaned_messages:\n",
    "        return None\n",
    "\n",
    "    # Apply chat template\n",
    "    chat_text = tokenizer.apply_chat_template(cleaned_messages, tokenize=False)\n",
    "    \n",
    "    # Tokenize the full conversation\n",
    "    tokenized = tokenizer(\n",
    "        chat_text, \n",
    "        add_special_tokens=False,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=None  # Return lists, not tensors\n",
    "    )\n",
    "    \n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:00<00:00, 1426.30 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:00<00:00, 3111.61 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:00<00:00, 94317.61 examples/s] \n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"json\", data_files=\"data/synthetic_dataset/synthetic_000.jsonl\", split=\"train\")\n",
    "\n",
    "def process_and_filter(example):\n",
    "    result = process(example)\n",
    "    return result is not None\n",
    "\n",
    "\n",
    "tokenised_ds = ds.map(process, remove_columns=ds.column_names, num_proc=4)\n",
    "tokenised_ds = tokenised_ds.filter(lambda x: len(x[\"input_ids\"]) > 0)  # Remove empty examples\n",
    "\n",
    "# Save the dataset\n",
    "Path(\"data/tokenized/cricket\").mkdir(parents=True, exist_ok=True)\n",
    "tokenised_ds.save_to_disk(\"data/tokenized/cricket\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smollm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
