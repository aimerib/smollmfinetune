{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roleplay MoE (Mixture‑of‑Experts) — full training script **and** a Hugging Face‑compatible\n",
    "`RoleplayMoEForCausalLM` so you can `AutoModelForCausalLM.from_pretrained(...)`\n",
    "and plug the mixture straight into `pipeline(\"text-generation\", ...)`.\n",
    "\n",
    "Usage overview\n",
    "──────────────\n",
    "Training (Jupyter or script):\n",
    "```python\n",
    "from roleplay_moe import train_full_moe, save_pretrained_moe\n",
    "\n",
    "moe, tokenizer = train_full_moe(\n",
    "    base_model=\"HuggingFaceTB/SmolLM2-135M\",\n",
    "    datasets=dict(\n",
    "        backstory=backstory_data,\n",
    "        personality=personality_data,\n",
    "        speech=speech_data,\n",
    "        nsfw=nsfw_data,\n",
    "    ),\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "save_pretrained_moe(moe, \"./moe_roleplay\", base_model=\"HuggingFaceTB/SmolLM2-135M\")\n",
    "```\n",
    "\n",
    "Inference just like any other HF model:\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
    "moe = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./moe_roleplay\", trust_remote_code=True, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "gen = pipeline(\"text-generation\", model=moe, tokenizer=tokenizer, device=0)\n",
    "print(gen(\"Tell me about your childhood.\", max_length=128)[0][\"generated_text\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PretrainedConfig,\n",
    "    PreTrainedModel,\n",
    "    GenerationMixin,\n",
    ")\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from safetensors.torch import save_file, load_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = {\n",
    "    \"additional_special_tokens\": [\n",
    "        \"<|prompt|>\",\n",
    "        \"<|backstory|>\", \n",
    "        \"<|personality|>\",\n",
    "        \"<|speech|>\",\n",
    "        \"<|nsfw|>\",\n",
    "        \"<|endofresponse|>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "class RoleplayDataset(Dataset):\n",
    "    \"\"\"Dataset for the new special token format.\"\"\"\n",
    "    \n",
    "    def __init__(self, formatted_text: str, tokenizer: AutoTokenizer, max_len: int = 256):\n",
    "        self.lines = [ln.strip() for ln in formatted_text.split(\"\\n\") if ln.strip()]\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Filter out just the text lines (not empty separator lines)\n",
    "        self.lines = [line for line in self.lines if line]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.lines[idx]\n",
    "        \n",
    "        # Tokenize with our special tokens\n",
    "        encoded = self.tok(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\", \n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {k: v.squeeze(0) for k, v in encoded.items()}\n",
    "\n",
    "class RoleplayMoE(nn.Module):\n",
    "    \"\"\"Improved MoE with better special token handling.\"\"\"\n",
    "    \n",
    "    def __init__(self, experts: List[nn.Module]):\n",
    "        super().__init__()\n",
    "        if not experts:\n",
    "            raise ValueError(\"Must pass >=1 experts\")\n",
    "        \n",
    "        self.experts = nn.ModuleList(experts)\n",
    "        d_model = experts[0].config.hidden_size\n",
    "        \n",
    "        # Gate network - takes embeddings and outputs expert weights\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(d_model, len(experts)),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Freeze expert parameters initially (only train gate)\n",
    "        for expert in self.experts:\n",
    "            for param in expert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, **kwargs):\n",
    "        # Get embeddings from first expert (they all share the same embedding layer structure)\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.experts[0].get_input_embeddings()(input_ids)\n",
    "        \n",
    "        # Gate computation: average embeddings across sequence length for each sample\n",
    "        # This gives us a representation of the whole input for routing\n",
    "        gate_input = embeddings.mean(dim=1)  # [batch_size, hidden_size]\n",
    "        expert_weights = self.gate(gate_input)  # [batch_size, num_experts]\n",
    "        \n",
    "        # Get outputs from each expert\n",
    "        logits = None\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            expert_logits = expert(input_ids, attention_mask=attention_mask).logits\n",
    "            \n",
    "            # Weight this expert's output by the gate\n",
    "            weight = expert_weights[:, i].unsqueeze(-1).unsqueeze(-1)  # [batch_size, 1, 1]\n",
    "            weighted_logits = expert_logits * weight\n",
    "            \n",
    "            if logits is None:\n",
    "                logits = weighted_logits\n",
    "            else:\n",
    "                logits = logits + weighted_logits\n",
    "        \n",
    "        return CausalLMOutputWithPast(logits=logits)\n",
    "    \n",
    "    def get_input_embeddings(self):\n",
    "        return self.experts[0].get_input_embeddings()\n",
    "    \n",
    "    def get_output_embeddings(self):\n",
    "        return self.experts[0].get_output_embeddings()\n",
    "\n",
    "class RoleplayMoEConfig(PretrainedConfig):\n",
    "    \"\"\"Config that serializes with `auto_map` so 🤗 knows how to reload custom code.\"\"\"\n",
    "\n",
    "    model_type = \"roleplay_moe\"\n",
    "\n",
    "    def __init__(self, base_model_name: str = \"\", num_experts: int = 1, **kwargs):\n",
    "        self.base_model_name = base_model_name\n",
    "        self.num_experts = num_experts\n",
    "        # Map Auto classes → symbols in model.py\n",
    "        self.auto_map = {\n",
    "            \"AutoConfig\": \"model.RoleplayMoEConfig\",\n",
    "            \"AutoModelForCausalLM\": \"model.RoleplayMoEForCausalLM\",\n",
    "        }\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class RoleplayMoEForCausalLM(GenerationMixin, PreTrainedModel):\n",
    "    config_class = RoleplayMoEConfig\n",
    "\n",
    "    def __init__(self, config: RoleplayMoEConfig):\n",
    "        super().__init__(config)\n",
    "        experts = [\n",
    "            AutoModelForCausalLM.from_pretrained(config.base_model_name)\n",
    "            for _ in range(config.num_experts)\n",
    "        ]\n",
    "        self.moe = RoleplayMoE(experts)\n",
    "\n",
    "    # Forward just delegates to inner MoE\n",
    "    def forward(self, input_ids, attention_mask=None, **kwargs):\n",
    "        return self.moe(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # GenerationMixin needs these helpers\n",
    "    def get_input_embeddings(self):\n",
    "        return self.moe.get_input_embeddings()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.moe.get_output_embeddings()\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "         return {\n",
    "             \"input_ids\": input_ids,\n",
    "             \"attention_mask\": kwargs.get(\"attention_mask\")\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tokenizer_and_model(base_model_name: str, device: str = \"cpu\"):\n",
    "    \"\"\"Load tokenizer and model, add special tokens, and resize embeddings.\"\"\"\n",
    "    \n",
    "    print(f\"Loading tokenizer and model: {base_model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    \n",
    "    # Set pad token if not present\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Add our special tokens\n",
    "    print(\"Adding special tokens...\")\n",
    "    num_added = tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "    print(f\"Added {num_added} special tokens to tokenizer\")\n",
    "    \n",
    "    # Load model and resize embeddings\n",
    "    model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "    if num_added > 0:\n",
    "        print(\"Resizing model embeddings...\")\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "def load_formatted_datasets(data_dir: str = \"data/formatted\") -> Dict[str, str]:\n",
    "    \"\"\"Load the converted datasets in special token format.\"\"\"\n",
    "    \n",
    "    datasets = {}\n",
    "    \n",
    "    for filename in [\"backstory.txt\", \"personality.txt\", \"speech.txt\", \"nsfw.txt\"]:\n",
    "        tag = filename.split(\".\")[0]\n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "        \n",
    "        if os.path.exists(filepath):\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                datasets[tag] = f.read()\n",
    "            print(f\"Loaded {tag} dataset from {filepath}\")\n",
    "        else:\n",
    "            print(f\"Warning: {filepath} not found, skipping {tag}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "def fine_tune_expert(\n",
    "    base_model_name: str,\n",
    "    formatted_data: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    epochs: int = 10,\n",
    "    lr: float = 5e-5,\n",
    "    batch_size: int = 8,\n",
    "    device: str = \"cpu\"\n",
    ") -> nn.Module:\n",
    "    \"\"\"Fine-tune a single expert on formatted data.\"\"\"\n",
    "    \n",
    "    # Create fresh model instance for this expert\n",
    "    model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "    \n",
    "    # Resize embeddings to match tokenizer (with special tokens)\n",
    "    if len(tokenizer) > model.config.vocab_size:\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = RoleplayDataset(formatted_data, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Setup training\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Shift for causal language modeling\n",
    "            shift_logits = logits[:, :-1].contiguous()\n",
    "            shift_labels = input_ids[:, 1:].contiguous()\n",
    "            \n",
    "            # Flatten for loss computation\n",
    "            flat_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "            flat_labels = shift_labels.view(-1)\n",
    "            \n",
    "            loss = criterion(flat_logits, flat_labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"  Epoch {epoch + 1}/{epochs}: Loss = {avg_loss:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def train_moe_gate(\n",
    "    moe: RoleplayMoE,\n",
    "    combined_data: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    epochs: int = 10,\n",
    "    lr: float = 1e-4,\n",
    "    batch_size: int = 8,\n",
    "    device: str = \"cpu\"\n",
    "):\n",
    "    \"\"\"Train the MoE gating network on combined data.\"\"\"\n",
    "    \n",
    "    moe.to(device)\n",
    "    \n",
    "    # Create dataset from combined formatted data\n",
    "    dataset = RoleplayDataset(combined_data, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Only optimize gate parameters\n",
    "    optimizer = torch.optim.AdamW(moe.gate.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        moe.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            \n",
    "            # Forward through MoE\n",
    "            outputs = moe(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Shift for causal language modeling\n",
    "            shift_logits = logits[:, :-1].contiguous()\n",
    "            shift_labels = input_ids[:, 1:].contiguous()\n",
    "            \n",
    "            # Flatten for loss computation\n",
    "            flat_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "            flat_labels = shift_labels.view(-1)\n",
    "            \n",
    "            loss = criterion(flat_logits, flat_labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(moe.gate.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"  MoE Epoch {epoch + 1}/{epochs}: Loss = {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Step 1: Prepare tokenizer with special tokens\n",
    "print(\"Step 1: Preparing tokenizer and base model...\")\n",
    "tokenizer, _ = prepare_tokenizer_and_model(base_model, device)\n",
    "\n",
    "# Step 2: Use formatted datasets created above\n",
    "print(\"\\nStep 2: Using formatted datasets...\")\n",
    "datasets = load_formatted_datasets()\n",
    "\n",
    "print(f\"Available datasets: {list(datasets.keys())}\")\n",
    "for tag, data in datasets.items():\n",
    "    line_count = len([line for line in data.split('\\n') if line.strip()])\n",
    "    print(f\"  {tag}: {line_count} entries\")\n",
    "\n",
    "# Step 3: Train individual experts\n",
    "print(\"\\nStep 3: Training individual experts...\")\n",
    "experts = []\n",
    "\n",
    "for tag, data in datasets.items():\n",
    "    print(f\"\\nTraining {tag} expert...\")\n",
    "    expert = fine_tune_expert(\n",
    "        base_model, data, tokenizer,\n",
    "        epochs=10, lr=5e-5, batch_size=8, device=device\n",
    "    )\n",
    "    experts.append(expert)\n",
    "\n",
    "# Step 4: Create MoE and train gate\n",
    "print(\"\\nStep 4: Training MoE gate network...\")\n",
    "moe = RoleplayMoE(experts)\n",
    "\n",
    "# Combine all formatted data for gate training\n",
    "combined_data = \"\\n\\n\".join(datasets.values())\n",
    "\n",
    "train_moe_gate(\n",
    "    moe, combined_data, tokenizer,\n",
    "    epochs=10, lr=1e-4, batch_size=8, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Save as a regular HF model directory ───────────────────────────────\n",
    "def write_model_file(save_dir: str):\n",
    "    \"\"\"Write the model.py file needed for HuggingFace auto-loading.\"\"\"\n",
    "    \n",
    "    model_code = '''import torch, torch.nn as nn\n",
    "from typing import List\n",
    "from transformers import AutoModelForCausalLM, PretrainedConfig, PreTrainedModel, GenerationMixin\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "class RoleplayMoE(nn.Module):\n",
    "    def __init__(self, experts: List[nn.Module]):\n",
    "        super().__init__()\n",
    "        assert experts\n",
    "        self.experts = nn.ModuleList(experts)\n",
    "        d = experts[0].config.hidden_size\n",
    "        self.gate = nn.Sequential(nn.Linear(d, len(experts)), nn.Softmax(dim=-1))\n",
    "        for p in self.experts.parameters(): \n",
    "            p.requires_grad = False\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.experts[0].get_input_embeddings()(input_ids)\n",
    "        gate_input = embeddings.mean(dim=1)\n",
    "        expert_weights = self.gate(gate_input)\n",
    "        \n",
    "        logits = None\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            expert_logits = expert(input_ids, attention_mask=attention_mask).logits\n",
    "            weight = expert_weights[:, i].unsqueeze(-1).unsqueeze(-1)\n",
    "            weighted = expert_logits * weight\n",
    "            logits = weighted if logits is None else logits + weighted\n",
    "        \n",
    "        return CausalLMOutputWithPast(logits=logits)\n",
    "    \n",
    "    def get_input_embeddings(self): \n",
    "        return self.experts[0].get_input_embeddings()\n",
    "    \n",
    "    def get_output_embeddings(self): \n",
    "        return self.experts[0].get_output_embeddings()\n",
    "\n",
    "class RoleplayMoEConfig(PretrainedConfig):\n",
    "    model_type = 'roleplay_moe'\n",
    "    \n",
    "    def __init__(self, base_model_name='', num_experts=1, **kwargs):\n",
    "        self.base_model_name = base_model_name\n",
    "        self.num_experts = num_experts\n",
    "        self.auto_map = {\n",
    "            'AutoConfig': 'model.RoleplayMoEConfig',\n",
    "            'AutoModelForCausalLM': 'model.RoleplayMoEForCausalLM',\n",
    "        }\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class RoleplayMoEForCausalLM(GenerationMixin, PreTrainedModel):\n",
    "    config_class = RoleplayMoEConfig\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        experts = [AutoModelForCausalLM.from_pretrained(config.base_model_name) \n",
    "                  for _ in range(config.num_experts)]\n",
    "        self.moe = RoleplayMoE(experts)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, **kwargs):\n",
    "        return self.moe(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    def get_input_embeddings(self): \n",
    "        return self.moe.get_input_embeddings()\n",
    "    \n",
    "    def get_output_embeddings(self): \n",
    "        return self.moe.get_output_embeddings()\n",
    "    \n",
    "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": kwargs.get(\"attention_mask\")\n",
    "        }\n",
    "'''\n",
    "    \n",
    "    with open(os.path.join(save_dir, \"model.py\"), \"w\") as f:\n",
    "        f.write(model_code)\n",
    "\n",
    "\n",
    "def save_moe_model(moe: RoleplayMoE, save_path: str, base_model_name: str, tokenizer: AutoTokenizer):\n",
    "    \"\"\"Save the trained MoE model in HuggingFace format.\"\"\"\n",
    "    \n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # Create wrapper for saving\n",
    "    config = RoleplayMoEConfig(\n",
    "        base_model_name=base_model_name,\n",
    "        num_experts=len(moe.experts)\n",
    "    )\n",
    "    \n",
    "    wrapper = RoleplayMoEForCausalLM(config)\n",
    "    wrapper.moe = moe\n",
    "    \n",
    "    # Save state dict\n",
    "    state_dict = {k: v.cpu() for k, v in wrapper.state_dict().items()}\n",
    "    save_file(state_dict, os.path.join(save_path, \"model.safetensors\"))\n",
    "    \n",
    "    # Save config\n",
    "    config.save_pretrained(save_path)\n",
    "    \n",
    "    # Save updated tokenizer (with special tokens)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    \n",
    "    # Create model.py file for loading\n",
    "    write_model_file(save_path)\n",
    "    \n",
    "    print(f\"✓ MoE model saved to {save_path}\")\n",
    "\n",
    "save_moe_model(moe, \"./moe_roleplay\", base_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_local_moe(path: str, device: str = \"cpu\") -> RoleplayMoEForCausalLM:\n",
    "    cfg = RoleplayMoEConfig.from_pretrained(path)\n",
    "    model = RoleplayMoEForCausalLM(cfg).to(device)\n",
    "    raw = load_file(os.path.join(path, \"model.safetensors\"))\n",
    "    model.load_state_dict(raw, strict=False)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "base_model = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "tokenizer   = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "# ── Load the MoE from the saved directory ───────────────────────────────\n",
    "#    trust_remote_code=True tells HF to load the custom RoleplayMoE class\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "moe_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./moe_roleplay\",           # folder created by save_pretrained_moe(...)\n",
    "    trust_remote_code=True,     # uses model.py in that folder\n",
    "    device_map=\"auto\"           # or .to(device) if you prefer manual control\n",
    ")\n",
    "\n",
    "moe_model.eval()\n",
    "\n",
    "# ── Quick generation test (built-in .generate) ──────────────────────────\n",
    "prompt = \"Tell me about your childhood.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    gen_ids = moe_model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,             # ← add this\n",
    "        max_length=128,\n",
    "        temperature=0.8,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(gen_ids[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smollm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
